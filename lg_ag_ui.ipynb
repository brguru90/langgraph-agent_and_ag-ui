{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d43325cf-ccc5-47dd-b1fa-c7a7801990fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Guruprasad.BR/workspace/RD/fds-documentation-explorer/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/Guruprasad.BR/workspace/RD/fds-documentation-explorer/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/Guruprasad.BR/workspace/RD/fds-documentation-explorer/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install langgraph\n",
    "%pip install ag-ui-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd0a9157-6ff4-4360-b5a5-0d0e60fe48a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"event\": \"on_chain_stream\",\n",
      "  \"run_id\": \"35abd94b-75f5-41b2-84ee-b0846a5049bb\",\n",
      "  \"name\": \"fds_agent\",\n",
      "  \"tags\": [],\n",
      "  \"metadata\": {\n",
      "    \"thread_id\": \"be45040a-aa5d-4451-9866-2645873e0e8c\",\n",
      "    \"user_id\": \"guru\"\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"chunk\": {\n",
      "      \"__interrupt__\": [\n",
      "        \"Interrupt(value=\\\"Tool call exceeded limit, please reply:\\\\n 1. 'yes' to continue\\\\n 2. 'no' to exit\\\\n 3. any other input will be treated as feedback prompt\\\", id='f16cfcb55ea1d8185d59a484fe69ae44')\"\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"parent_ids\": []\n",
      "}\n",
      "1352\n"
     ]
    }
   ],
   "source": [
    "import pickle,os, traceback,json\n",
    "\n",
    "all_events=[]\n",
    "with open(\"all_events.pkl\", \"rb\") as file:\n",
    "    all_events=pickle.load(file)\n",
    "# \n",
    "print(json.dumps(all_events[-2],default=str,indent=2))\n",
    "print(len(all_events))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c968463",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dc27f7a-089e-4e59-a1fe-f812de155d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Verbose\n",
      "1352\n",
      "99812b71-d969-43eb-a427-569df61b5440 tool_use_89f8ff82-8b86-4d1a-a553-d9d70613b2b7\n",
      "cc1e10fe-f569-48bb-9254-72606d898897 tool_use_23af5481-e036-424e-8a1a-04a4fbccdcc1\n",
      "60d92686-dba4-457b-9ee5-9b46216cae28 text_041d24bd-06cc-4f46-89c5-17f179e56cde\n",
      "21440071-274b-4ba0-8e0d-89a77ab7de2f tool_use_60d92686-dba4-457b-9ee5-9b46216cae28\n",
      "1f90a8ac-afee-4469-bd05-b8be716a877b tool_use_eb270320-03e5-4a7d-afba-cf4b4fe4c75c\n",
      "c2033438-0b95-43d5-990b-1b46edcf0262 tool_use_652078bf-af49-4660-9532-9a5c605f6149\n",
      "a5038778-aa3f-441d-80b8-3d947152b252 tool_use_f7f23a2e-d7ca-4100-8f5a-4222d0dd2632\n",
      "fc5a303b-920c-444c-87e6-5d79ec6ef56e tool_use_ec88b85a-5565-4006-a24f-9403dca7756c\n",
      "02cd8a48-d307-4a6f-8e9a-41db3bc7629c tool_use_861c0370-618d-4f04-82f2-08b203952bbc\n",
      "No active wrap status to end: ended\n",
      "event_types {'on_chain_start': 47, 'on_chain_stream': 38, 'on_chain_end': 40, 'on_chat_model_start': 9, 'on_chat_model_stream': 1193, 'on_chat_model_end': 9, 'on_tool_start': 8, 'on_tool_end': 8}\n",
      "chunk_types {'on_chat_model_stream': {'text', 'tool_use'}, 'on_tool_start': 8, 'on_tool_end': 8, 'on_chain_stream': 1}\n",
      "parsed_events {'on_chat_model_stream': 1193, 'on_tool_start': 8, 'on_tool_end': 8, 'on_chain_stream': 1}\n",
      "unparsed_events {'on_chain_start': 47, 'on_chain_stream': 37, 'on_chain_end': 40, 'on_chat_model_start': 9, 'on_chat_model_stream': 1193, 'on_chat_model_end': 9}\n",
      "Final Events Count: 1200\n"
     ]
    }
   ],
   "source": [
    "%xmode Verbose\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import copy\n",
    "\n",
    "from typing import Dict,List,TypedDict,Any,Optional\n",
    "from enum import Enum\n",
    "import pickle,os, traceback,json\n",
    "from langgraph.types import Command,Interrupt\n",
    "from langchain_core import messages as langchain_messages\n",
    "from datetime import datetime, timezone\n",
    "from ag_ui.core import EventType\n",
    "from ag_ui.encoder import EventEncoder\n",
    "\n",
    "from ag_ui.core import (\n",
    "    EventType,\n",
    "    CustomEvent,\n",
    "    MessagesSnapshotEvent,\n",
    "    RawEvent,\n",
    "    RunAgentInput,\n",
    "    RunErrorEvent,\n",
    "    RunFinishedEvent,\n",
    "    RunStartedEvent,\n",
    "    StateDeltaEvent,\n",
    "    StateSnapshotEvent,\n",
    "    StepFinishedEvent,\n",
    "    StepStartedEvent,\n",
    "    TextMessageContentEvent,\n",
    "    TextMessageEndEvent,\n",
    "    TextMessageStartEvent,\n",
    "    ToolCallArgsEvent,\n",
    "    ToolCallEndEvent,\n",
    "    ToolCallStartEvent,\n",
    "    ThinkingTextMessageStartEvent,\n",
    "    ThinkingTextMessageContentEvent,\n",
    "    ThinkingTextMessageEndEvent,\n",
    "    ThinkingStartEvent,\n",
    "    ThinkingEndEvent,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# event_types {'on_chain_start': 9, 'on_chain_stream': 16, 'on_chain_end': 9, 'on_chat_model_start': 3, 'on_chat_model_stream': 566, 'on_chat_model_end': 3, 'on_tool_start': 2, 'on_tool_end': 2}\n",
    "\n",
    "\n",
    "class EventTrackType(str, Enum):\n",
    "    REGISTERED = \"registered\"\n",
    "    START = \"start\"\n",
    "    END = \"end\"\n",
    "\n",
    "\n",
    "class TrackEvent(TypedDict):\n",
    "    state: EventTrackType\n",
    "    type: EventType\n",
    "\n",
    "\n",
    "class LangGraphToAgUi:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ref={\"wrap_status\":None,\"last_type\":None,\"last_id\":None}\n",
    "        self.tool_use_id=None\n",
    "\n",
    "        self.event_types={}\n",
    "        self.chunk_types={}\n",
    "        self.parsed_events={}\n",
    "        self.unparsed_events={}\n",
    "        self.filtered_event=None\n",
    "        self.track_event:Dict[str,TrackEvent]={} # mostly all events are sequential except tool calls\n",
    "\n",
    "    def __get_filtered_data(self,event):\n",
    "        if \"event\" in event:\n",
    "            self.track_event[event[\"run_id\"]]=self.track_event.get(event[\"run_id\"],{})\n",
    "            _type=event[\"event\"]\n",
    "            self.event_types[_type]=self.event_types.get(_type,0)+1\n",
    "            _contents=None\n",
    "            if isinstance(event.get(\"data\",{}).get(\"chunk\",{}),langchain_messages.BaseMessage):\n",
    "                ai_msg:langchain_messages.AIMessage=event.get(\"data\",{}).get(\"chunk\",{})\n",
    "                _contents=ai_msg.content\n",
    "                self.unparsed_events[_type]=self.unparsed_events.get(_type,0)+1            \n",
    "            elif event.get(\"data\") and event[\"data\"].get(\"output\") and type(event[\"data\"][\"output\"])==Command:  \n",
    "                self.unparsed_events[_type]=self.unparsed_events.get(_type,0)+1           \n",
    "                return None\n",
    "            elif (event.get(\"data\") and event[\"data\"].get(\"chunk\") and type(event[\"data\"][\"chunk\"])==Command):\n",
    "                self.unparsed_events[_type]=self.unparsed_events.get(_type,0)+1   \n",
    "                return None\n",
    "            elif event.get(\"data\",{}).get(\"chunk\",{}).get(\"__interrupt__\",False):\n",
    "                _interrupt:Interrupt= event[\"data\"][\"chunk\"][\"__interrupt__\"]\n",
    "                _contents={\n",
    "                    \"type\": \"interrupt\",\n",
    "                    \"value\":_interrupt[0].value,\n",
    "                    \"id\": _interrupt[0].id,\n",
    "                }            \n",
    "            elif _type in [\"on_tool_start\",\"on_tool_end\"]:\n",
    "                pass\n",
    "            elif not isinstance(event.get(\"data\",{}).get(\"chunk\",{}),langchain_messages.BaseMessage):\n",
    "                if event.get(\"data\",{}).get(\"chunk\",{}).get(\"start_conv\",True):\n",
    "                    self.unparsed_events[_type]=self.unparsed_events.get(_type,0)+1   \n",
    "                    return None\n",
    "                if event.get(\"data\",{}).get(\"chunk\",{}).get(\"llm\",True):\n",
    "                    self.unparsed_events[_type]=self.unparsed_events.get(_type,0)+1   \n",
    "                    return None\n",
    "                if event.get(\"data\",{}).get(\"chunk\",{}).get(\"route\",True):\n",
    "                    self.unparsed_events[_type]=self.unparsed_events.get(_type,0)+1   \n",
    "                    return None      \n",
    "                print(\"Unhandled\", event)    \n",
    "            elif event.get(\"data\") and event[\"data\"].get(\"input\") and event[\"data\"][\"input\"].get(\"store\"):\n",
    "                event[\"data\"][\"input\"][\"store\"] = \"Accessing to store information\"   \n",
    "            else:\n",
    "                _contents=event.get(\"data\",{}).get(\"chunk\",{}).get(\"content\")\n",
    "            self.parsed_events[_type]=self.parsed_events.get(_type,0)+1\n",
    "            if _type==\"on_chat_model_stream\" and _contents is not None and type(_contents) is list:\n",
    "                for _content in _contents:\n",
    "                    if \"type\" in _content:\n",
    "                        self.chunk_types[_type]=self.chunk_types.get(_type,set()) | {_content[\"type\"]}\n",
    "            else:\n",
    "                self.chunk_types[_type]=self.chunk_types.get(_type,0)+1\n",
    "\n",
    "        return event\n",
    "\n",
    "    \n",
    "    def __process_chunk(self,event,_type,_content):\n",
    "        last_type=self.ref.get(\"last_type\",_type)\n",
    "        last_id:str=self.ref.get(\"last_id\",event[\"run_id\"]) or event[\"run_id\"]\n",
    "        if not last_id.endswith(event[\"run_id\"]):\n",
    "            print(event[\"run_id\"],last_id)\n",
    "        ac_events=[]\n",
    "        if ((_type!= last_type and last_type is not None) or not last_id.endswith(event[\"run_id\"])) and self.ref[\"wrap_status\"]==\"started\":\n",
    "            if last_type==\"text\":\n",
    "                ac_events.append(TextMessageEndEvent(\n",
    "                    type=EventType.TEXT_MESSAGE_END,\n",
    "                    message_id=self.ref[\"last_id\"] or \"\"\n",
    "                ))\n",
    "            elif last_type==\"reasoning_content\":\n",
    "                ac_events.append(CustomEvent(\n",
    "                    type=EventType.CUSTOM,\n",
    "                    name=last_type,\n",
    "                    value={\"text\":\"\",\"type\":EventType.THINKING_TEXT_MESSAGE_END,\"message_id\":self.ref[\"last_id\"] or \"\"}\n",
    "                ))\n",
    "            elif last_type==\"tool_use\":\n",
    "                ac_events.append(CustomEvent(\n",
    "                    type=EventType.CUSTOM,\n",
    "                    name=last_type,\n",
    "                    value={\"text\":\"\",\"type\":EventType.TEXT_MESSAGE_END,\"message_id\":self.ref[\"last_id\"] or \"\"}\n",
    "                ))\n",
    "            self.ref[\"wrap_status\"]=\"ended\"\n",
    "            self.ref[\"last_id\"]=None\n",
    "        if ((_type!= last_type) or not last_id.endswith(event[\"run_id\"])) and self.ref[\"wrap_status\"] != \"started\":\n",
    "            if _type==\"text\":\n",
    "                self.ref[\"last_id\"]=\"text_\"+event[\"run_id\"]\n",
    "                ac_events.append(TextMessageStartEvent(\n",
    "                    type=EventType.TEXT_MESSAGE_START,\n",
    "                    role='assistant',\n",
    "                    message_id=self.ref[\"last_id\"] or \"\"\n",
    "                ))\n",
    "            elif _type==\"reasoning_content\":\n",
    "                self.ref[\"last_id\"]=\"reasoning_content_\"+event[\"run_id\"]\n",
    "                ac_events.append(CustomEvent(\n",
    "                    type=EventType.CUSTOM,\n",
    "                    name=_type,\n",
    "                    value={\"text\":\"\",\"type\":EventType.THINKING_TEXT_MESSAGE_START,\"message_id\":self.ref[\"last_id\"] or \"\"},\n",
    "                ))\n",
    "            elif _type==\"tool_use\":\n",
    "                self.ref[\"last_id\"]=\"tool_use_\"+event[\"run_id\"]\n",
    "                ac_events.append(CustomEvent(\n",
    "                    type=EventType.CUSTOM,\n",
    "                    name=_type,\n",
    "                    value={\"text\":\"\",\"type\":EventType.TEXT_MESSAGE_START,\"message_id\":self.ref[\"last_id\"] or \"\"},\n",
    "                ))\n",
    "            self.ref[\"wrap_status\"]=\"started\"    \n",
    "        if _content:\n",
    "            if _type==\"text\":\n",
    "                if _content.get(\"text\"):\n",
    "                    ac_events.append(TextMessageContentEvent(\n",
    "                        type=EventType.TEXT_MESSAGE_CONTENT,\n",
    "                        message_id=\"text_\"+event[\"run_id\"],\n",
    "                        # raw_event=event,\n",
    "                        delta= _content.get(\"text\"),\n",
    "                    ))\n",
    "            elif _type==\"tool_use\":\n",
    "                if _content.get(\"id\") and _content.get(\"name\"):\n",
    "                    ac_events.append(CustomEvent(\n",
    "                        type=EventType.CUSTOM,\n",
    "                        # raw_event=event,\n",
    "                        name=_type,\n",
    "                        value= {\"text\":f\"Proposed Tool Call: Name: {_content[\"name\"]}, Id: {_content[\"id\"]}\",\"type\":EventType.TEXT_MESSAGE_CONTENT,\"message_id\":\"tool_use_\"+event[\"run_id\"]}\n",
    "                    ))\n",
    "                elif _content.get(\"input\"):\n",
    "                    ac_events.append(CustomEvent(\n",
    "                        type=EventType.CUSTOM,\n",
    "                        # raw_event=event,\n",
    "                        name=_type,\n",
    "                        value= {\"text\":f\"Arguments: {_content[\"input\"]}\",\"type\":EventType.TEXT_MESSAGE_CONTENT,\"message_id\":\"tool_use_\"+event[\"run_id\"]}\n",
    "                    ))   \n",
    "            elif _type==\"reasoning_content\":\n",
    "                reasoning_text=_content.get(\"reasoning_content\",{}).get(\"text\")\n",
    "                if reasoning_text:\n",
    "                    ac_events.append(CustomEvent(\n",
    "                        type=EventType.CUSTOM,\n",
    "                        name=_type,\n",
    "                        value= {\"text\":reasoning_text,\"type\":EventType.THINKING_TEXT_MESSAGE_CONTENT,\"message_id\":\"reasoning_content_\"+event[\"run_id\"]}\n",
    "                    ))   \n",
    "            else:\n",
    "                print(\"Unhandled chunk type:\", _type, _content)\n",
    "        return ac_events\n",
    "    \n",
    "    def __process_chunks(self,event):\n",
    "        chunks=[]\n",
    "        if event[\"event\"]==\"on_chat_model_stream\": # if its still chunk, close other type if any before creating new chunk\n",
    "            base_message:langchain_messages.BaseMessage=event.get(\"data\",{}).get(\"chunk\",{})  \n",
    "            _contents=base_message.content\n",
    "            for i in range(len(_contents)):\n",
    "                _content=_contents[i]\n",
    "                if \"type\" in _content:\n",
    "                    chunks.extend(self.__process_chunk(event, _content[\"type\"], _content))\n",
    "                    self.ref[\"last_type\"] = _content[\"type\"]\n",
    "        elif self.ref[\"wrap_status\"]==\"started\": # since this function called first, handle end of text or tool_use use for any other event\n",
    "            allow_close=self.ref[\"last_type\"] in [\"text\",\"tool_use\"]\n",
    "            if allow_close:\n",
    "                chunks.extend(self.__process_chunk(event, event[\"event\"], None))\n",
    "                self.ref[\"last_type\"]=None\n",
    "                self.ref[\"wrap_status\"]=\"ended\"\n",
    "                self.ref[\"last_id\"]=None\n",
    "        return chunks\n",
    "        \n",
    "    def __process_non_chunks(self,event):\n",
    "        ac_events=[]\n",
    "        if not isinstance(event.get(\"data\",{}).get(\"chunk\",{}),langchain_messages.BaseMessage):\n",
    "            if event[\"event\"]==\"on_tool_start\":\n",
    "                self.ref[\"wrap_status\"]=\"started\"\n",
    "                self.ref[\"last_type\"]=\"tool_call\"\n",
    "                ac_events.append(ToolCallStartEvent(\n",
    "                    type=EventType.TOOL_CALL_START,\n",
    "                    tool_call_name=event[\"name\"],\n",
    "                    tool_call_id=event[\"run_id\"] or\"\",\n",
    "                    raw_event=event\n",
    "                ))\n",
    "                ac_events.append(ToolCallArgsEvent(\n",
    "                    type=EventType.TOOL_CALL_ARGS,\n",
    "                    delta=json.dumps(event[\"data\"][\"input\"], default=str),\n",
    "                    tool_call_id=event[\"run_id\"] or \"\",\n",
    "                    raw_event=event\n",
    "                ))\n",
    "            elif event[\"event\"]==\"on_tool_end\":\n",
    "                self.ref[\"wrap_status\"]=\"ended\"\n",
    "                self.ref[\"last_id\"]=None\n",
    "                self.ref[\"last_type\"]=\"tool_call\"\n",
    "                tool_message:langchain_messages.ToolMessage=event[\"data\"][\"output\"]\n",
    "                ac_events.append(ToolCallEndEvent(\n",
    "                    type=EventType.TOOL_CALL_END,\n",
    "                    tool_call_id=event[\"run_id\"] or \"\",\n",
    "                    raw_event=event\n",
    "                ))\n",
    "            elif event.get(\"data\",{}).get(\"chunk\",{}).get(\"__interrupt__\",False):\n",
    "                _interrupt:Interrupt= event[\"data\"][\"chunk\"][\"__interrupt__\"]\n",
    "                ac_events.append(CustomEvent(\n",
    "                    type=EventType.CUSTOM,\n",
    "                    name=\"on_interrupt\",\n",
    "                    value={\n",
    "                        \"text\": _interrupt[0].value,\n",
    "                        \"id\": _interrupt[0].id,\n",
    "                        \"type\": \"on_interrupt\"\n",
    "                    },\n",
    "                    raw_event=event\n",
    "                ))\n",
    "            else:\n",
    "                print(\"Unhandled event:\", event)\n",
    "        return ac_events\n",
    "\n",
    "    async def __handle_event(self,event):\n",
    "        chunks=[]\n",
    "        chunks.extend(self.__process_chunks(event))\n",
    "        chunks.extend(self.__process_non_chunks(event))           \n",
    "        if chunks:\n",
    "            for chunk in chunks:\n",
    "                yield chunk\n",
    "\n",
    "    async def transform_events(self,event):  # !event: don't modify event it will have reference\n",
    "        # event=copy.deepcopy(event) # !Caution, avoid modifying langgraph data, since it will have reference to all its internal variables\n",
    "        _event=self.__get_filtered_data(event)\n",
    "        self.filtered_event=_event\n",
    "        if _event is None:\n",
    "            yield None\n",
    "        else:\n",
    "            async for transformed_event in self.__handle_event(_event):\n",
    "                yield transformed_event\n",
    "\n",
    "    def __end_events(self):\n",
    "        if self.ref[\"wrap_status\"]==\"started\":\n",
    "            if self.ref[\"last_type\"]==\"text\":\n",
    "                return TextMessageEndEvent(\n",
    "                    type=EventType.TEXT_MESSAGE_END,\n",
    "                    message_id=self.ref[\"last_id\"] or \"\"\n",
    "                )\n",
    "            elif self.ref[\"last_type\"]==\"tool_use\":\n",
    "                return CustomEvent(\n",
    "                    type=EventType.CUSTOM,\n",
    "                    name=self.ref[\"last_type\"],\n",
    "                    value={\"text\":\"\",\"type\":EventType.TEXT_MESSAGE_END,\"message_id\":self.ref[\"last_id\"] or \"\"}\n",
    "                )\n",
    "            else:\n",
    "                print(\"Unhandled end event type:\", self.ref[\"last_type\"])\n",
    "                return RawEvent(\n",
    "                    type=EventType.RAW,\n",
    "                    event=f\"Unhandled end event type: {self.ref['last_type']}\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"No active wrap status to end:\", self.ref[\"wrap_status\"])\n",
    "            return RawEvent(\n",
    "                type=EventType.RAW,\n",
    "                event=\"No active wrap status to end\"\n",
    "            )\n",
    "\n",
    "    def end_events(self):\n",
    "        data=self.__end_events()\n",
    "        print(\"event_types\", self.event_types)\n",
    "        print(\"chunk_types\", self.chunk_types)\n",
    "        print(\"parsed_events\", self.parsed_events)\n",
    "        print(\"unparsed_events\", self.unparsed_events)\n",
    "        self.ref= {\"wrap_status\": None, \"last_type\": None}\n",
    "        return data\n",
    "    \n",
    "ag_transformers = LangGraphToAgUi()\n",
    "try:\n",
    "    encoder = EventEncoder(accept=\"text/event-stream\")\n",
    "    final_events=[]\n",
    "    filtered_event=[]\n",
    "    print(len(all_events))\n",
    "    with open(\"original_events.json\", \"w\") as file:\n",
    "        file.write(json.dumps(all_events, default=str,indent=2))\n",
    "    for event in all_events:\n",
    "        processed_events=ag_transformers.transform_events(event)\n",
    "        filtered_event.append(ag_transformers.filtered_event)\n",
    "        async for transformed_event in processed_events:\n",
    "            if transformed_event:\n",
    "                final_events.append(encoder.encode(transformed_event))\n",
    "    final_events.append(encoder.encode(ag_transformers.end_events()))\n",
    "    \n",
    "    print(\"Final Events Count:\", len(final_events))\n",
    "    with open(\"filtered_data.json\", \"w\") as file:\n",
    "        file.write(json.dumps(filtered_event, default=str,indent=2))\n",
    "    with open(\"final_events.json\", \"w\") as file:\n",
    "        file.write(json.dumps(final_events, default=str,indent=2))\n",
    "except Exception as e:\n",
    "    traceback.print_exc(limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adac496-7919-4c72-b023-c75aef17357e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fe940-ccc1-43c4-966f-abe2d556de11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fds-doc (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
